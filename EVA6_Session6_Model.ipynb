{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA6_Session6_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONpQVaYMJ+7i8zg/BHgr9J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bharatgirdhar/EVA6_Session6/blob/main/EVA6_Session6_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q98pYp8VrOMi"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxyqD6I6rBRQ"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vNormalizationType):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.dropout=nn.Dropout(0.05)\n",
        "        self.conv1 = nn.Conv2d(1, 8, 3, padding=1,bias=False) #28\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm1=nn.GroupNorm(2,8)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm1=nn.LayerNorm([8,28,28],elementwise_affine=False)\n",
        "        else:\n",
        "          self.Norm1=nn.BatchNorm2d(8)\n",
        "\n",
        "        #self.conv2 = nn.Conv2d(8, 8, 3, padding=1,bias=False)#28 \n",
        "        #self.BatchNorm2=nn.BatchNorm2d(8)\n",
        "        self.conv3 = nn.Conv2d(8, 16, 3, padding=1,bias=False)#28\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm3=nn.GroupNorm(2,16)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm3=nn.LayerNorm([16,28,28],elementwise_affine=False)        \n",
        "        else:\n",
        "          self.Norm3=nn.BatchNorm2d(16)\n",
        "\n",
        "        #self.BatchNorm3=nn.BatchNorm2d(16)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)#14        \n",
        "        self.conv4 = nn.Conv2d(16, 8, 1, bias=False)#14\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm4=nn.GroupNorm(2,8)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm4=nn.LayerNorm([8,14,14],elementwise_affine=False)\n",
        "        else:\n",
        "          self.Norm4=nn.BatchNorm2d(8)\n",
        "        \n",
        "        #self.BatchNorm4=nn.BatchNorm2d(8)\n",
        "        #self.conv5 = nn.Conv2d(8, 8, 3,bias=False)#12\n",
        "        #self.BatchNorm5=nn.BatchNorm2d(8)\n",
        "        self.conv6 = nn.Conv2d(8, 8, 3, bias=False)#12\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm6=nn.GroupNorm(2,8)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm6=nn.LayerNorm([8,12,12],elementwise_affine=False)\n",
        "        else:\n",
        "          self.Norm6=nn.BatchNorm2d(8)\n",
        "\n",
        "        \n",
        "        #self.BatchNorm6=nn.BatchNorm2d(8)\n",
        "        self.conv7 = nn.Conv2d(8, 16, 3, bias=False)#10\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm7=nn.GroupNorm(2,16)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm7=nn.LayerNorm([16,10,10],elementwise_affine=False)\n",
        "        else:\n",
        "          self.Norm7=nn.BatchNorm2d(16)\n",
        "\n",
        "        #self.BatchNorm7=nn.BatchNorm2d(16)\n",
        "        self.conv8 = nn.Conv2d(16, 8, 1, bias=False)#10\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm8=nn.GroupNorm(2,8)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm8=nn.LayerNorm([8,10,10],elementwise_affine=False)\n",
        "        else:\n",
        "          self.Norm8=nn.BatchNorm2d(8)\n",
        "\n",
        "        #self.BatchNorm8=nn.BatchNorm2d(8)\n",
        "        self.conv9 = nn.Conv2d(8, 16, 3, bias=False)#8\n",
        "        #self.BatchNorm9=nn.BatchNorm2d(16)\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm9=nn.GroupNorm(2,16)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm9=nn.LayerNorm([16,8,8],elementwise_affine=False)\n",
        "        else:\n",
        "          self.Norm9=nn.BatchNorm2d(16)\n",
        "\n",
        "        self.conv10 = nn.Conv2d(16, 32, 3, bias=False)#6\n",
        "        #self.BatchNorm10=nn.BatchNorm2d(32)\n",
        "        if vNormalizationType==0:\n",
        "          self.Norm10=nn.GroupNorm(2,32)\n",
        "        elif vNormalizationType==1:\n",
        "          self.Norm10=nn.LayerNorm([32,6,6],elementwise_affine=False)\n",
        "        else:\n",
        "          self.Norm10=nn.BatchNorm2d(32)\n",
        "\n",
        "        self.gap1=nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.fc1=nn.Linear(in_features=32, out_features=10)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.pool1(self.dropout(F.relu(self.Norm3(self.conv3(self.dropout(F.relu(self.Norm1(self.conv1(x)))))))))\n",
        "        #print(x.shape)\n",
        "        x = self.dropout(F.relu(self.Norm7(self.conv7(self.dropout(F.relu(self.Norm6(self.conv6(self.dropout(F.relu(self.Norm4(self.conv4(x))))))))))))\n",
        "        #print(x.shape)\n",
        "        x = F.relu(self.Norm10(self.conv10(self.dropout(F.relu(self.Norm9(self.conv9(self.dropout(F.relu(self.Norm8(self.conv8(x)))))))))))\n",
        "        #x = self.conv7(x)\n",
        "        #print(x.shape)\n",
        "        x = self.gap1(x)\n",
        "        \n",
        "        x = x.view(-1, 32)\n",
        "        #print(x.shape)\n",
        "        x=self.fc1(x)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 2,
      "outputs": []
    }
  ]
}